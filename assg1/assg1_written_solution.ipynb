{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q1 - softmax\n",
    "\n",
    "### (a) Prove that softmax is invariant to constant oﬀsets in the input. $ softmax(x) = softmax(x + c) $\n",
    "\n",
    "note: In practice, we make use of this property and choose $c = − max_i x_i$ when computing softmax probabilities for numerical stability (i.e., subtracting its maximum element from all elements of x).\n",
    "\n",
    "证明：\n",
    "$$\n",
    "softmax(x + c)_i = \\frac{e^{x_i + c}}{\\Sigma_j{e^{x_j + c}}} \n",
    "= \\frac{e^c . e^{x_i}}{e^c . \\Sigma_j{e^{x_j}}}\n",
    "= \\frac{e^{x_i}}{\\Sigma_j{e^{x_j}}} = softmax(x)_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### broadcasting in numpy\n",
    "\n",
    "q1.b 为编程题，见 `q1_softmax.py`。下面的例子用来熟悉 numpy 的 broadcasting。\n",
    "\n",
    "numpy 的 array 运算，通常是 element-wise。broadcasting 的意义在于减少不必要的数据拷贝，提高运算效率。\n",
    "\n",
    "理解 broadcasting 规则最好的方法，就是把参与运算的两个 array 的 shape 右对齐列出来，等于1的维度就要被拉伸。比如：\n",
    "\n",
    "```\n",
    "A     :  4 x 1 x 5 x 1\n",
    "B     :      3 x 1 x 2\n",
    "Result:  4 x 3 x 5 x 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(4)\n",
    "xx = x.reshape(4,1)\n",
    "y = np.ones(5)\n",
    "z = np.ones((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((4,), (5,))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,) (5,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0d2092a2fa4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,) (5,) "
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((4, 1), (4,))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.],\n",
       "       [ 2.,  2.,  2.,  2.,  2.],\n",
       "       [ 3.,  3.,  3.,  3.,  3.],\n",
       "       [ 4.,  4.,  4.,  4.,  4.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(xx.shape, x.shape)\n",
    "xx + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((4,), (3, 4))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  4.],\n",
       "       [ 1.,  2.,  3.,  4.],\n",
       "       [ 1.,  2.,  3.,  4.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.shape, z.shape)\n",
    "x + z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer operation\n",
    "\n",
    "Broadcasting provides a convenient way of taking outer operations (outer product, outer addtion, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   2.,   3.],\n",
       "       [ 11.,  12.,  13.],\n",
       "       [ 21.,  22.,  23.],\n",
       "       [ 31.,  32.,  33.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0., 10., 20., 30.])\n",
    "b = np.array([1., 2., 3.])\n",
    "a[:, np.newaxis] + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q2 - neural network basics\n",
    "\n",
    "### (a) Derive the gradients of the sigmoid function and show that it can be rewritten as a function of the function value\n",
    "\n",
    "$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $\n",
    "\n",
    "**求导：**\n",
    "\n",
    "$ \\sigma'(x) = - (1 + e^{-x})^{-2} (- e^{-x}) = \\sigma(x) \\frac{e^{-x}}{1 + e^{-x}} = \\sigma(x) (1 - \\sigma(x)) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Derive the gradient with regard to the inputs of a softmax function when cross entropy loss is used for evaluation\n",
    "\n",
    "$$ \\hat{y} = softmax(\\theta) $$\n",
    "$$ CE(y, \\hat{y}) \n",
    "= -\\Sigma{y_i log(\\hat{y}_i)} \n",
    "= -\\Sigma{y_i log(softmax(\\theta)_i)}\n",
    "$$  \n",
    "\n",
    "* y is one-hot vector\n",
    "* 注意这里 $\\theta$ 是 softmax input vector，而不是参数\n",
    "\n",
    "**求解：**\n",
    "\n",
    "记 $ softmax(x) = s(x) $  \n",
    "与 sigmoid 类似，可求得：\n",
    "$$ \\frac{\\partial{s(\\theta_i)}}{\\partial{\\theta_i}} = s(\\theta_i)\\cdot(1 - s(\\theta_i)) $$\n",
    "$$ \\frac{\\partial{s(\\theta_k)}}{\\partial{\\theta_i}} = -s(\\theta_k)\\cdot s(\\theta_i),  k\\neq i $$\n",
    "\n",
    "设正确的 class 为 k (即 $y_k = 1$ )，则：\n",
    "$$ CE(y, \\hat{y}) \n",
    "= -log(softmax(\\theta)_k)\n",
    "$$\n",
    "\n",
    "当 $k = i$ 时：\n",
    "$$ \\frac{\\partial{CE(y, \\hat{y})}}{\\partial{\\theta_i}}\n",
    "= -\\frac{\\partial{s(\\theta_i)}/\\partial{\\theta_i}}{s(\\theta_i)}\n",
    "= s(\\theta_i) - 1 = \\hat{y}_i - 1\n",
    "$$\n",
    "当 $k \\neq i$ 时：\n",
    "$$ \\frac{\\partial{CE(y, \\hat{y})}}{\\partial{\\theta_i}}\n",
    "= -\\frac{\\partial{s(\\theta_k)}/\\partial{\\theta_i}}{s(\\theta_k)}\n",
    "= - s(\\theta_i) = \\hat{y}_i\n",
    "$$\n",
    "综上，\n",
    "$$ \\frac{\\partial{CE(y, \\hat{y})}}{\\partial{\\theta}}\n",
    "= \\hat{y} - y\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) 单隐层神经网络交叉熵损失函数的梯度\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) 参数个数\n",
    "\n",
    "$ (Dx + 1) * H + (H + 1) * Dy $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q3 - word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q4 - sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "225px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
